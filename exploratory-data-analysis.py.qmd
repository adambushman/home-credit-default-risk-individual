---
title: "Home Credit Default Risk"
subtitle: "Part 1 | Exploratory Data Analysis | IS 6812"
author: "Adam Bushman (u6049169)"
date: "9/10/2024"
format: 
    html:
        css: styles.css
        theme: simplex
        toc: true
        embed-resources: true
editor:
    render-on-save: true
---

# Introduction

## The Project

#### Business Problem

Home Credit cares deeply about the population without sufficient credit history and aims to improve inclusion for this underrepresented group by designing prediction models for loan repayment, such that capable borrowers are not denied solely due to absence of credit while protecting against defaults.

#### Benefit of a Solution

A more accurate prediction model will help Home Credit a) provide lending terms and b) sufficiently balance risk such that financial inclusion is expanded.

#### Success Metrics

The success of the project will be measured in accurately predicting loan defaults, so that an increase in loan approval rates among this underrepresented group is accompanied by a stable or even reduction in default rates.

#### Analytics Approach

A supervised machine learning classification approach will be used to predict loan default risk, leveraging alternative data sources such as telco and transactional information, with the target variable being loan repayment.

#### Scope

The project will deliver a predictive model for assessing loan repayment abilities.

#### Details

The project will be executed by a team of business analysts, equipped with data processing and modeling skills. The project will feature iterative milestones for data exploration, model development, and evaluation, culminating in a final model by calendar year end.


## My Approach

For this exploratory data analysis, I plan to use the following languages, APIs, and tools:

*   [Quarto](https://quarto.org/): probably "the" notebook of choice for the scientific community; built and maintained by Posit, it's flexible, powerful, and beautiful.
*   [Python](https://www.python.org/): I usually default to R for analytical tasks so I could use the extra practice in Python.
*   [DuckDB](https://duckdb.org/): an in-process analytical database engine. It is extremely fast and features a convenient syntax. It's gathered tremendous steam in the industry, even for analysis tasks.

::: {layout="[[3,4,4,5]]" layout-valign="center"}
![](https://quarto-dev.github.io/quarto-r/logo.png)

![](https://quantumzeitgeist.com/wp-content/uploads/pythoned.png)

![](https://duckdb.org/images/logo-dl/DuckDB_Logo.png)
::::

Within the Python ecosystem I'll use several libraries that will augment the analytical process. These include, but are not limited to:

*   [pandas](https://pandas.pydata.org/docs/index.html): probably *the* foundational package for data analysis in Python
*   [scikit-learn](https://scikit-learn.org/stable/): a comprehensive API for data science models and workflows
*   [statsmodels](https://www.statsmodels.org/stable/index.html): one of the best statistics libraries in Python
*   [plotnine](https://plotnine.org/): the {ggplot2} equivalent in Python, bringing "grammar of graphics"
*   [skimpy](https://pypi.org/project/skimpy/): a neat package that started in R designed to summarize tabular data in a digestible way


::: {layout="[[8,14,5,5]]" layout-valign="center"}
![](https://numfocus.org/wp-content/uploads/2016/07/pandas-logo-300.png)

![](https://miro.medium.com/v2/resize:fit:984/1*OTt5dpD5N4Ru1qn0WNsZ4g.png)

![](https://plotnine.org/images/logo-540.png)

![](https://docs.ropensci.org/skimr/logo.png)
::::

# Analysis Preparation

## Data Source

The data is sourced from [Kaggle](https://www.kaggle.com/competitions/home-credit-default-risk/overview) where [Home Credit](https://www.homecredit.net/) hosted a competitiion back in the summer of 2018. Data was downloaded therefrom on August 20th, 2024 and used concurrently throughout the Capstone course at University of Utah, Fall 2024.


## Loading Data

We'll start off importing `duckdb`. We'll do much of the data processing work using this powerful SQL engine.

```{python}

import duckdb

```

With access to the API, we can begin to query our data located in files. We need only setup "relations" (informing `duckdb` where the files are located).

```{python .duck-db-relations}
#| output: false

# DuckDB relations
# These are tables against which we can query using the DuckDB API

duckdb.read_csv("data/application_test.csv")
duckdb.read_csv("data/application_train.csv")
duckdb.read_csv("data/bureau_balance.csv")
duckdb.read_csv("data/bureau.csv")
duckdb.read_csv("data/credit_card_balance.csv")
duckdb.read_csv("data/installments_payments.csv")
duckdb.read_csv("data/POS_CASH_balance.csv")
duckdb.read_csv("data/previous_application.csv")

```

This approach isn't what you'd call "conventional". How are we expecting to work with These data? We'll walk through some use cases quickly to demonstrate how seamless it will actually be.


## Working With the Data

Straight away, we can interact with these files with plain SQL. For example, with a simple query (below) we can look at the first 10 rows of the `bureau.csv` file.

```{python}

duckdb.sql("SELECT * FROM 'data/bureau.csv' LIMIT 10").show()

```

DuckDB does a nice job styling the output and including helpful details, such as data types.

But don't think we're "stuck" in the world of SQL for the duration of this analysis; far from it. DuckDB is very adept at refactoring the data for use with the other packages we'll be using. Let's bring in `pandas` and see how this works:

```{python}

import pandas as pd

```

```{python}

df = duckdb.sql("SELECT * FROM 'data/bureau.csv' LIMIT 10").fetchdf()

type(df)

```

As you can see, we now have a `pandas` data frame. We could run a model with `scikit-learn`, generate a visualization with `plotnine`, or perform complex, custom logic ill-fit for SQL. For now, we'll just select every other column:

```{python}

df_sub = df.loc[:,::2]

df_sub.info()

```

At this point, we may be ready to leverage the speed and efficiency of DuckDB. So we can just switch right back!

```{python}

duckdb.sql("SELECT * FROM df_sub").show()

```

There really won't be any issue navigating between the different APIs. In fact, we'll be able to leverage the unique strengths of each of them to best advance our analysis. 


# Exploration

## Skim each dataset

We'll start off by getting familiar with each of the data sets. We'll use the `{skimpy}` package for this and do some visualizations. We'll rely on the ERD to help with interpretation:

![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)

### `application_train.csv`

>   Main tables - out train and test samples; target (binary); info about loan and loan applicant at application time.

This file is the same as `application_test.csv` except that it features the target variable.

```{python}
#| class: scrolling-y

from skimpy import skim

app_train_df = (
    duckdb                                                 # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/application_train.csv'")     # Read the file
    .fetchdf()                                             # Convert to a pandas dataframe
)

skim(app_train_df)                                         # "Skim" the data set

```

This is a LARGE data set. Let's break down some of the observations we can glean from the summary:

#### Data Types

*   There's several variables casted as numeric that should be categorical
    *   Any variables with `FLAG_` prefixes: ~32 columns
        *   Examples: `FLAG_MOBIL`, `FLAG_DOCUMENT_#`, ...
    *   Other variables treated as flags but not specified in the name: ~6 columns
        *   Examples: `REG_REGION_NOT_LIVE_REGION`, `LIVE_CITY_NOT_WORK_CITY`, ...
    *   `REGION_RATING_` variables that are ordinal classificaitons
*   Many variables of type `string` could be candidates for categorical depending on count of unique values
    *   Examples: `NAME_CONTRACT_TYPE`, `ORGANIZATION_TYPE`
*   Some variables casted as `string` but may need to be numeric
    *   Any with `_MODE` suffix

#### Missing Data

*   `EXT_SOURCE_#` these are scores ingested from third parties
    *   Understandably sparse; not every client would be represented in third-party systems
*   Fields including `APARTMENT_`, `BASEMENT_`, `ELEVATOR_`, etc., all relate to the client's current dwelling.
    *   43 of the file's 122 columns relate to the client's dwelling
    *   These are very sparse, about ~50% complete. These fields represent most of the missing data
    *   Presumably, the lender was unable to collect such detail for every client
*   `OBS_##_SOCIAL_CIRCLE` and `DEF_##_SOCIAL_CIRCLE` relate to default rates on observation ## days past due
    *   This is a count that's heavily skewed to the right
    *   <1% of observations have these fields as missing
*   `AMT_REQ_CREDIT_BUREAU_XX` are fields measure the number of credit inqueries in XX timeframe
    *   These data set is geared around users without sufficient credit history so makes sense this is missing
    *   ~13% of the data is missing
*   `OCCUPATION_TYPE` refers to the type of job the user has
    *   Missing for virtually 1/3 clients
    *   `ORGANIZATION_TYPE`, however, is not missing so we do retain some attribute relative to their work
    *   Perhaps in combination with `NAME_EDUCATION_TYPE` there's predictive power

#### Distribution

*   All dwelling related variables have been *normalized* based on description column descriptions file
    *   Checking the histograms to the far right, this appears to be accurate
*   There's a couple variables with skewed distributions
    *   `DAYS_REGISTRATION`: skewed right
    *   `DAYS_LAST_PHONE_CHANGE`: skewed right

#### Potential feature engineering

*   Dimensionality reduction
    *   `FLAG_DOCUMENT_#` fields could potentially be summarized a few ways;
        *   % of documents completed
        *   Finding principal components
        *   Maybe only certain documents matter
    *   There's many measures of central tendency for dwelling (avg, mode, median)
        *   Likely not all for each perspective is needed

### `bureau.csv`

>   Application data from previous loans that client got from other institutions that were reported to the Credit Bureau. One row per client's loan in Credit Bureau

This file will tell us all about the previous loans any credit history client would have. Let's digest the data:

```{python}
#| class: scrolling-y

from skimpy import skim

bureau_df = (
    duckdb                                      # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/bureau.csv'")     # Read the file
    .fetchdf()                                  # Convert to a pandas dataframe
)

skim(bureau_df)                                 # "Skim" the data set

```

The first thing to notice is the ~1.7M records, compared to the previous file of ~360K. Cearly there will be multiple historical bureau records for each matching application.

#### Data Types

*   All seems to be in fair order
*   There's opportunity with the 3 `string` variables to cast as a `categorical`

#### Missing Data

*   5 columns are missing data
    *  `DAYS_CREDIT_ENDDATE` and `DAYS_ENDDATE_FACT` measure days between current application and end date of bureau credit; the end date must just be missing
    *   `AMT_CREDIT_MAX_OVERDUE` is curiously missing a lot of data; perhaps due to there not having been an overdue balance to report
    *   `AMT_CREDIT_SUM_DEBT` is understandable if someone had no current debt sums
    *   `AMT_CREDIT_SUM_LIMIT` is understandably blank should a client not have a credit card
    *   `AMT_ANNUITY` only those with an annuity loan would have values here

Many of those seem resonable to populate with zeros in a cleaning phase.


#### Distribution

*   `DAYS_CREDIT` seem to be the only significantly skewed variable


#### Potential feature engineering

*   I'd be curious just how much variability in credit reports per individual could be explained by dimensionality reduction (PCA)


### `bureau_balance.csv`

>   Monthly balance of credits in Credit Bureau

Essentially, we have an expanded version of balances per client loan reported to the bureau.

```{python}
#| class: scrolling-y

from skimpy import skim

bureau_bal_df = (
    duckdb                                              # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/bureau_balance.csv'")     # Read the file
    .fetchdf()                                          # Convert to a pandas dataframe
)

skim(bureau_bal_df)                                     # "Skim" the data set

```

The first thing I'm seeing is ~27.2M records. Multiple monthly balances per loan.

#### Data Types

*   All seems to be in fair order
*   There's opportunity the `status` variable to cast as a `categorical`

#### Missing Data

No missing data


#### Distribution

*   `MONTHS_BLANCE` seems to be skewed to the right


#### Potential feature engineering

*   It's possible this dataset can be represented in 2 variables:
    *   Existence of previous loan: boolean
    *   Median oustanding balance: numeric



### `previous_application.csv`

>   Application data of client's previous loans in Home Credit. Info about the previous loan parameters and client info at time of previous application. One row per previous application.

This file details previous applications with Home Credit. Some clients may have never applied for loans previously while others could have had multiple applications.

```{python}
#| class: scrolling-y

from skimpy import skim

prev_app_df = (
    duckdb                                                      # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/previous_application.csv'")       # Read the file
    .fetchdf()                                                  # Convert to a pandas dataframe
)

skim(prev_app_df)                                               # "Skim" the data set

```

This file has ~1.0M records. Understandably, there's more records than applications due to monthly summaries and some customers having multiple previous loans.

#### Data Types

*   A few variables are mapped as continuous but need to be moved to discrete (possibly categorical)
    *   `SELLERPLACE_AREA`, `NFLAG_INSURED_ON_APPROVAL`, `NFLAG_MICRO_CASH`, `NFLAG_LAST_APPL_IN_DAY`, `FLAG_LAST_APPL_PER_CONTRACT`
*   A few variables mapped as `string` may be better suited as categorical
    *   `NAME_CONTRACT_TYPE`, `NAME_CONTRACT_STATUS`, `NAME_PAYMENT_TYPE`, `NAME_CLIENT_TYPE`, `NAME_GOODS_CATEGORY`, etc.

#### Missing Data

*   There are several missing data points
    *   `AMT_ANNUITY` refers to the previous application; if there was none or its balance is zero, makes sense to be missing
    *   `AMT_DOWN_PAYMENT` would be zero if there was none made
    *   `RATE_DOWN_PAYMENT` how much was put down relative to the loan (see above)
    *   `RATE_INTEREST_PRIMARY` & `RATE_INTEREST_PRIVILEGED`; since this is applicationss, it could be there was previous credit
        *   A high rate of missing data (99%)
    *   `CNT_PAYMENT` how far into a previous loan for the current one
    *   `DAYS_FIRST_*` refers to the first dispersement, due amount, etc; if no loan was approved, makes sense to be blank
    *   `NAME_TYPE_SUITE` is the only discrete field with high NAs (49%); it indicates who accompanied the client in for the application


#### Distribution

*   Only two right-skewed data points I can see
    *   `DAYS_DECISION` and `RATE_INTEREST_PRIVILEGED` (virtually no data on this one)


#### Potential feature engineering

*  Ultimately this will need to be aggregated; ways to represent the data?
    *   Count of previous applications
    *   Status/reject rates
    *   Classify goods, maybe (% reasonable)



### `installments_payments.csv`

>   Past payment data for each installments of previous credits in Home Credit related to loans in our sample.

These data will link to the current application and to past applications. Some clients will have previous loans, even multiple, while others may have none. We can see payments due and made in this file.

```{python}
#| class: scrolling-y

from skimpy import skim

installment_pmt_df = (
    duckdb                                                      # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/installments_payments.csv'")      # Read the file
    .fetchdf()                                                  # Convert to a pandas dataframe
)

skim(installment_pmt_df)                                        # "Skim" the data set

```

This file has ~1.4M records. 

#### Data Types

*   We've got two ID columns that aren't really continuous: `SK_ID_PREV` and `SK_ID_CURR`
*   All other variable data types look fine

#### Missing Data

*   `DAYS_ENTR_PAYMENT` and `AMT_PAYMENT` are mostly complete. The few missing values must be unpaid amounts, may due to default but perhaps due to the cutoff date


#### Distribution

*   `DAYS_INSTALMENT` seems to be skewed to the right


#### Potential feature engineering

*   We could probably synthesize this down to the installment payment timing



### `POS_CASH_balance.csv`

>   Monthly balance of client's previous loans in Home Credit

These data will link to the application and to past payment installments. Some clients will have previous loans, even multiple, while other may have none.

```{python}
#| class: scrolling-y

from skimpy import skim

cash_bal_df = (
    duckdb                                              # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/POS_CASH_balance.csv'")   # Read the file
    .fetchdf()                                          # Convert to a pandas dataframe
)

skim(cash_bal_df)                                       # "Skim" the data set

```

This file has ~1.0M records. Understandably, there's more records than applications due to monthly summaries and some customers having multiple previous loans.

#### Data Types

*   We've got two ID columns that aren't really continuous: `SK_ID_PREV` and `SK_ID_CURR`
*   There's opportunity the `NAME_CONTRACT_STATUS` variable to cast as a `categorical`

#### Missing Data

*   `CNT_INSTALLMENT` and `CNT_INSTALLMENT_FUTURE` are mostly complete. Where there exist missing values, it means there's nothing outstanding in previous loans.


#### Distribution

*   `MONTHS_BLANCE` seems to be skewed to the right


#### Potential feature engineering

*   It's possible this dataset can be represented in 2 variables:
    *   Existence of previous loan: boolean
    *   Median oustanding balance: numeric



### `credit_card_balance.csv`

>   Monthly balance of client's previous credit card loans in Home Credit

These data are all about credit card loans with Home Credit, specifically the balance that is carried.

```{python}
#| class: scrolling-y

from skimpy import skim

credit_card_df = (
    duckdb                                                  # Use DuckDB's query engine
    .sql("SELECT * FROM 'data/credit_card_balance.csv'")    # Read the file
    .fetchdf()                                              # Convert to a pandas dataframe
)

skim(credit_card_df)                                        # "Skim" the data set

```

This file has ~3.8M records. 

#### Data Types

*   We've got two ID columns that aren't really continuous: `SK_ID_PREV` and `SK_ID_CURR`
*   There's opportunity the `NAME_CONTRACT_STATUS` variable to cast as a `categorical`

#### Missing Data

*   There are several "count" variables with missing date (~20% missing)
    *   `CNT_INSTALLMENT_MATURE_CUM`, `CNT_DRAWINGS_POS_CURRENT`, `CNT_DRAWINGS_OTHER_CURRENT`, `CNT_DRAWINGS_ATM_CURRENT`
    *   I assume much of these are missing given there were now withdrawals for the categories
*   Some "amount" variables corresponding to the same categories above missing similar amounts of data


#### Distribution

*   `CNT_INSTALLMENT_MATURE_CUM` is very left skewed (understandable with a cumulative measure)
*   `MONTHS_BALANCE` is right skewed


#### Potential feature engineering

*   It's possible this dataset can be represented in 2 variables:
    *   Withdrawal amounts relative to limit
    *   Withdrawal counts
